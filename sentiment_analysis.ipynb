{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1e8c077",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c5c5ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.float_format', lambda x: '%.3f' % x)\n",
    "pd.set_option('display.width', 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8407fab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_df(dataframe):\n",
    "    \"\"\"\n",
    "    Checks the overall structure and key metrics of a DataFrame.\n",
    "\n",
    "    Args:\n",
    "        dataframe (pd.DataFrame): DataFrame to inspect.\n",
    "\n",
    "    Returns:\n",
    "        None: Prints shape, data types, head, tail, missing values, and quantiles.\n",
    "    \"\"\"\n",
    "    print(\"##################### Shape #####################\")\n",
    "    print(dataframe.shape)\n",
    "    print(\"##################### Types #####################\")\n",
    "    print(dataframe.dtypes)\n",
    "    print(\"##################### Head #####################\")\n",
    "    print(dataframe.head(5))\n",
    "    print(\"##################### Tail #####################\")\n",
    "    print(dataframe.tail(5))\n",
    "    print(\"##################### NA #####################\")\n",
    "    print(dataframe.isnull().sum())\n",
    "    print('##################### Unique Values #####################')\n",
    "    print(dataframe.nunique())\n",
    "    print(\"##################### Duplicates #####################\")\n",
    "    print(dataframe.duplicated().sum())\n",
    "    print(\"##################### Quantiles #####################\")\n",
    "    # Uncomment below to include quantile information\n",
    "    #print(dataframe[[col for col in dataframe.columns if dataframe[col].dtypes != \"O\"]].quantile([0, 0.05, 0.50, 0.75, 0.95, 0.99, 1]).T)\n",
    "    print(dataframe.describe().T)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70be243e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_train():\n",
    "    df_train = pd.read_csv(\"data/train.csv\", encoding=\"UTF-8\", engine=\"python\", encoding_errors=\"replace\")#replaces damaged bytes with \"\\ufffd\"\n",
    "    return df_train\n",
    "\n",
    "def load_test():\n",
    "    df_test = pd.read_csv(\"data/test.csv\", encoding=\"UTF-8\", engine=\"python\", encoding_errors=\"replace\")\n",
    "    return df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e15f244c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = load_train()\n",
    "df_test = load_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fda9fad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "check_df(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b783f213",
   "metadata": {},
   "outputs": [],
   "source": [
    "check_df(df_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12376162",
   "metadata": {},
   "source": [
    "- damaged rows filtering, these can be considered to be dropped\n",
    "- also other (#NAME?) damage can be seen during data read, these rows will be dropped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a1f10f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "damaged_rows_train = df_train[df_train[\"text\"].str.contains(\"\\ufffd\", na=False)]\n",
    "damaged_rows_test = df_test[df_test[\"text\"].str.contains(\"\\ufffd\", na=False)]\n",
    "\n",
    "print(f\"Total damaged rows in train: {len(damaged_rows_train)}\")\n",
    "\n",
    "print(damaged_rows_train.head())\n",
    "\n",
    "print(f\"Total damaged rows in test: {len(damaged_rows_test)}\")\n",
    "\n",
    "print(damaged_rows_test.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1031daf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.drop(index=damaged_rows_train.index, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ee79b63",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_train[df_train['text'] != \"#NAME?\"]\n",
    "df_test = df_test[df_test['text'] != \"#NAME?\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "548c2712",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in df_train.columns:\n",
    "    df_train[col] = df_train[col].str.lower() # Normalizing Case Folding\n",
    "    df_train[col] = df_train[col].str.replace(r'[^\\w\\s]', '', regex=True) # Punctuations\n",
    "    df_train[col] = df_train[col].str.replace(r'\\d+', '', regex=True) # Numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f6e791b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in df_test.columns:\n",
    "    df_test[col] = df_test[col].str.lower() # Normalizing Case Folding\n",
    "    df_test[col] = df_test[col].str.replace(r'[^\\w\\s]', '', regex=True) # Punctuations\n",
    "    df_test[col] = df_test[col].str.replace(r'\\d+', '', regex=True) # Numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfd5647e",
   "metadata": {},
   "outputs": [],
   "source": [
    "check_df(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "836ee396",
   "metadata": {},
   "outputs": [],
   "source": [
    "check_df(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "602703ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.drop_duplicates(inplace=True)\n",
    "df_test.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "432d409c",
   "metadata": {},
   "outputs": [],
   "source": [
    "check_df(df_train)\n",
    "check_df(df_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faec8354",
   "metadata": {},
   "source": [
    "**TASK**\n",
    "\n",
    "4 different models ([TF-IDF with Multinomial Naive Bayes and Binary Naive Bayes] + [ANN with Word2Vec and FastText]) will be trained and compared."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86ff79b7",
   "metadata": {},
   "source": [
    "**ROADMAP**\n",
    "\n",
    "Preprocessing steps will be applied on data according to models they will be fed to.\n",
    "\n",
    "***For Bayesian Model:***\n",
    "- Lowecase transformation\n",
    "- Special characters cleaning (Punctuations etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "499d0ba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def concat_df_on_y_axis(df_1, df_2):\n",
    "    \"\"\"\n",
    "    Concatenates two DataFrames along the Y-axis (rows).\n",
    "\n",
    "    Args:\n",
    "        df_1 (pd.DataFrame): First DataFrame.\n",
    "        df_2 (pd.DataFrame): Second DataFrame.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Concatenated DataFrame.\n",
    "    \"\"\"\n",
    "    return pd.concat([df_1, df_2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61a587fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_test = concat_df_on_y_axis(df_train, df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1998d13",
   "metadata": {},
   "outputs": [],
   "source": [
    "check_df(df_train_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c816f75",
   "metadata": {},
   "source": [
    "**OBSERVATIONS**\n",
    "- df_train has 0 duplicates, duplicates dropped.\n",
    "- df_test has 0 duplicates, duplicates dropped.\n",
    "- df_train_test has 515 duplicates.\n",
    "- **Data Leakage observed**\n",
    "- Set of {df_train INTERSECT df_test} has to be removed from df_train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e5f0833",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_texts = set(df_test['text'])\n",
    "df_train = df_train[~df_train['text'].isin(test_texts)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "650dff71",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_test = concat_df_on_y_axis(df_train, df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a35c1f59",
   "metadata": {},
   "outputs": [],
   "source": [
    "check_df(df_train_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02c65be0",
   "metadata": {},
   "source": [
    "**Data Leakage problem solved**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31ca3654",
   "metadata": {},
   "source": [
    "## Naive Bayes Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34564fd0",
   "metadata": {},
   "source": [
    "**STOPWORDS REMOVAL**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bafd1330",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fe655ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('stopwords')\n",
    "\n",
    "sw = stopwords.words('turkish')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2adad87",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_test_sw_removed = df_train_test.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "670c917c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_test_sw_removed['text'] = df_train_test_sw_removed['text'].apply(lambda x: \" \".join(x for x in str(x).split() if x not in sw))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ab6920c",
   "metadata": {},
   "source": [
    "**STEMMING**\n",
    "- Stemming is easy and will produce enough efficiency with bayesian models\n",
    "- Lemmatization can be alternative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf3417d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from TurkishStemmer import TurkishStemmer\n",
    "stemmer = TurkishStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78b1e10d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_test_sw_removed['text'] = df_train_test_sw_removed['text'].apply(lambda x: \" \".join([stemmer.stem(word) for word in x.split()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3b860ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "check_df(df_train_test_sw_removed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69a1f00b",
   "metadata": {},
   "outputs": [],
   "source": [
    "len_train = len(df_train)\n",
    "\n",
    "df_train_sw_removed_stemmed = df_train_test_sw_removed.iloc[:len_train].copy()\n",
    "\n",
    "df_test_sw_removed_stemmed = df_train_test_sw_removed.iloc[len_train:].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d88a0f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = df_train_sw_removed_stemmed['text']\n",
    "y_train = df_train_sw_removed_stemmed['label']\n",
    "X_test = df_test_sw_removed_stemmed['text']\n",
    "y_test = df_test_sw_removed_stemmed['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b80f6b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caaee6c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB, BernoulliNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65dd20ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f185b8a",
   "metadata": {},
   "source": [
    "### Multinomial Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "548e3002",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(ngram_range=(1,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03045a86",
   "metadata": {},
   "source": [
    "**TF-IDF Vectorization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eb60f61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for multinomial NB\n",
    "X_train_nb = tfidf_vectorizer.fit_transform(X_train)\n",
    "X_test_nb = tfidf_vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be7de2b3",
   "metadata": {},
   "source": [
    "**Multinomial NB Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab9f2234",
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_model = MultinomialNB().fit(X_train_nb, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88576a58",
   "metadata": {},
   "source": [
    "**Multinomial NB Model Evaluation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e51c22f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_model_pred = nb_model.predict(X_test_nb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edbb23d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, nb_model_pred, digits=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ae615c1",
   "metadata": {},
   "source": [
    "### Binary Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3017372",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer_binary = TfidfVectorizer(ngram_range=(1,2), binary=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26e57425",
   "metadata": {},
   "source": [
    "**Binary TF-IDF Vectorization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8affbe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for binary NB\n",
    "X_train_nb_binary = tfidf_vectorizer_binary.fit_transform(X_train)\n",
    "X_test_nb_binary = tfidf_vectorizer_binary.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a07c2fa",
   "metadata": {},
   "source": [
    "**Binary NB Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b0e70f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_model_binary = BernoulliNB().fit(X_train_nb_binary, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "295ab00d",
   "metadata": {},
   "source": [
    "**Binary NB Model Evaluation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13684c0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_binary_model_pred = nb_model.predict(X_test_nb_binary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc1f8187",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, nb_binary_model_pred, digits=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fd57429",
   "metadata": {},
   "source": [
    "## ANN MODELÄ°NG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de2ca016",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('punkt_tab') #necessary for tokenization method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c301254b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizing sentences\n",
    "tokenized_sentences_train = [word_tokenize(sentence.lower()) for sentence in df_train['text']]\n",
    "tokenized_sentences_test = [word_tokenize(sentence.lower()) for sentence in df_test['text']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb29719f",
   "metadata": {},
   "source": [
    "### SKIPGRAM-ANN MODEL\n",
    "- Skipgram has better performance modeling semantics, which we desperately need in this case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6459c0a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ee7daec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "EMBEDDING_DIM = 10      # Increased from 5 to 100 for better representation\n",
    "WINDOW_SIZE = 2\n",
    "MAX_VOCAB_SIZE = 20000   # Limit vocabulary to top 20k words to prevent OOM errors\n",
    "BATCH_SIZE = 128         \n",
    "NUM_EPOCHS = 10       \n",
    "LEARNING_RATE = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02312676",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = tokenized_sentences_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d7ae975",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten the list of sentences to a single list of words\n",
    "all_words = [word for sentence in sentences for word in sentence]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54e4cb9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select only the most common words to keep the vocabulary size manageable\n",
    "# We reserve index 0 for <UNK>, so we take MAX_VOCAB_SIZE - 1\n",
    "word_counts = Counter(all_words).most_common(MAX_VOCAB_SIZE - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d9f980d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create vocabulary mapping: <UNK> is always at index 0\n",
    "word_to_ix = {\"<UNK>\": 0}\n",
    "for word, count in word_counts:\n",
    "    word_to_ix[word] = len(word_to_ix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c5035b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create reverse mapping (Index -> Word)\n",
    "ix_to_word = {ix: word for word, ix in word_to_ix.items()}\n",
    "VOCAB_SIZE = len(word_to_ix)\n",
    "\n",
    "print(f\"Total words scanned: {len(all_words)}\")\n",
    "print(f\"Final Vocabulary Size: {VOCAB_SIZE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d478d88e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Skip-gram Pairs (Input -> Target)\n",
    "inputs = []\n",
    "targets = []\n",
    "\n",
    "print(\"Generating training pairs...\")\n",
    "for sentence in sentences:\n",
    "    # Convert words to indices. If a word is not in top 20k, it becomes 0 (<UNK>)\n",
    "    sentence_indices = [word_to_ix.get(word, 0) for word in sentence]\n",
    "    \n",
    "    for i in range(len(sentence_indices)):\n",
    "        target_word_idx = sentence_indices[i] # Center word\n",
    "        \n",
    "        # Optimization: If the target word is unknown (<UNK>), \n",
    "        # we skip training on it to avoid noise.\n",
    "        if target_word_idx == 0:\n",
    "            continue\n",
    "            \n",
    "        # Define context window\n",
    "        start_idx = max(0, i - WINDOW_SIZE)\n",
    "        end_idx = min(len(sentence_indices), i + WINDOW_SIZE + 1)\n",
    "        \n",
    "        for j in range(start_idx, end_idx):\n",
    "            if i != j: # Skip the target word itself\n",
    "                context_word_idx = sentence_indices[j]\n",
    "                inputs.append(target_word_idx)\n",
    "                targets.append(context_word_idx)\n",
    "\n",
    "print(f\"Total training pairs generated: {len(inputs)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9655fcfa",
   "metadata": {},
   "source": [
    "#### Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71aa8217",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import site\n",
    "\n",
    "try:\n",
    "    site_packages = site.getsitepackages()[0]\n",
    "    nvidia_path = os.path.join(site_packages, 'nvidia')\n",
    "    \n",
    "    cudnn_path = os.path.join(nvidia_path, 'cudnn', 'lib')\n",
    "    cuda_path = os.path.join(nvidia_path, 'cuda_runtime', 'lib')\n",
    "    \n",
    "    old_ld = os.environ.get('LD_LIBRARY_PATH', '')\n",
    "    os.environ['LD_LIBRARY_PATH'] = f\"{cudnn_path}:{cuda_path}:{old_ld}\"\n",
    "    \n",
    "    # This specific flag often fixes 'DNN library initialization failed' errors\n",
    "    # by disabling some auto-tuning features that might crash on certain GPUs.\n",
    "    os.environ['TF_CUDNN_USE_AUTOTUNE'] = '0' \n",
    "    \n",
    "    print(\"NVIDIA Library paths arranged successfully\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Path warning: {e}\")\n",
    "\n",
    "# --- 2. IMPORT TENSORFLOW AND CONFIGURE GPU MEMORY ---\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.keras import layers, models, optimizers, callbacks\n",
    "\n",
    "print(f\"TensorFlow Version: {tf.__version__}\")\n",
    "\n",
    "# GPU Memory Growth\n",
    "# This is CRITICAL. It prevents TensorFlow from hogging all VRAM at start-up.\n",
    "# Must be run immediately after importing TF.\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        print(f\"GPU Detected and memory growth set: {gpus}\")\n",
    "    except RuntimeError as e:\n",
    "        print(f\"GPU Error: {e}\")\n",
    "else:\n",
    "    print(\"No GPU detected.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a8b5228",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert lists to NumPy arrays (TensorFlow prefers typed arrays)\n",
    "inputs = np.array(inputs, dtype=np.int32)\n",
    "targets = np.array(targets, dtype=np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4ee96aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use tf.data.Dataset for efficient Batching and Prefetching on GPU\n",
    "dataset = tf.data.Dataset.from_tensor_slices((inputs, targets))\n",
    "\n",
    "# Shuffle buffer size should ideally be >= number of training samples\n",
    "# Prefetch allows the CPU to prepare the next batch while GPU processes the current one\n",
    "dataset = dataset.shuffle(buffer_size=1024).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c386bf9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_skipgram_model(vocab_size, embedding_dim):\n",
    "    # Input layer: Receives a single integer (word index)\n",
    "    input_word = layers.Input(shape=(1,), name=\"target_word_input\")\n",
    "    \n",
    "    # Embedding layer: Converts index to dense vector\n",
    "    # input_dim: Vocabulary size\n",
    "    # output_dim: Size of the vector space\n",
    "    x = layers.Embedding(input_dim=vocab_size, \n",
    "                         output_dim=embedding_dim, \n",
    "                         input_length=1, \n",
    "                         name=\"embedding_layer\")(input_word)\n",
    "    \n",
    "    # Flatten: Converts (Batch, 1, Dim) -> (Batch, Dim)\n",
    "    x = layers.Flatten()(x)\n",
    "    \n",
    "    # Output layer: Predicts probability for every word in vocabulary\n",
    "    # Softmax ensures output sums to 1 (probability distribution)\n",
    "    output = layers.Dense(vocab_size, activation='softmax', name=\"context_prediction\")(x)\n",
    "    \n",
    "    model = models.Model(inputs=input_word, outputs=output)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "703ddb03",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WordSimilarityCallback(callbacks.Callback):\n",
    "    \"\"\"\n",
    "    A custom callback to print the most similar words to a specific query word\n",
    "    at the end of each epoch. This helps in monitoring the semantic learning progress.\n",
    "    \"\"\"\n",
    "    def __init__(self, test_word, word_to_ix, ix_to_word, top_k=5):\n",
    "        super(WordSimilarityCallback, self).__init__()\n",
    "        self.test_word = test_word\n",
    "        self.word_to_ix = word_to_ix\n",
    "        self.ix_to_word = ix_to_word\n",
    "        self.top_k = top_k\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        # Check if the test word is in vocabulary\n",
    "        if self.test_word not in self.word_to_ix:\n",
    "            return\n",
    "\n",
    "        # 1. Retrieve the weights from the embedding layer\n",
    "        embedding_layer = self.model.get_layer(\"embedding_layer\")\n",
    "        embeddings = embedding_layer.get_weights()[0]\n",
    "        \n",
    "        # 2. Get the vector for the test word\n",
    "        test_idx = self.word_to_ix[self.test_word]\n",
    "        test_vector = embeddings[test_idx]\n",
    "        \n",
    "        # 3. Calculate Cosine Similarity\n",
    "        # Normalize embeddings and the test vector to unit length\n",
    "        norm_embeddings = tf.math.l2_normalize(embeddings, axis=1)\n",
    "        norm_test_vector = tf.math.l2_normalize(test_vector, axis=0)\n",
    "        \n",
    "        # Dot product of normalized vectors equals cosine similarity\n",
    "        cosine_similarities = tf.tensordot(norm_embeddings, norm_test_vector, axes=1)\n",
    "        \n",
    "        # 4. Find the indices of the words with the highest similarity scores\n",
    "        # We take top_k + 1 because the most similar word is the word itself (score=1.0)\n",
    "        top_indices = tf.math.top_k(cosine_similarities, k=self.top_k + 1).indices.numpy()\n",
    "        \n",
    "        # 5. Print the results\n",
    "        closest_words = [self.ix_to_word[idx] for idx in top_indices if idx != test_idx]\n",
    "        print(f\"\\n[Validation] End of Epoch {epoch+1} - Closest words to '{self.test_word}':\")\n",
    "        print(f\"  -> {', '.join(closest_words)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f1752eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model\n",
    "model = build_skipgram_model(VOCAB_SIZE, EMBEDDING_DIM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "112d130a",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_k_metric = tf.keras.metrics.SparseTopKCategoricalAccuracy(k=5, name='top_5_acc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d275c98d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We use 'sparse_categorical_crossentropy' because our targets are integers (indexes),\n",
    "# not one-hot encoded vectors. This saves memory and is computationally efficient.\n",
    "model.compile(optimizer=optimizers.Adam(learning_rate=LEARNING_RATE),\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=[top_k_metric])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f04fca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the custom callback (e.g., check neighbors of \"learning\")\n",
    "# Note: Ensure the test_word exists in your training data\n",
    "visual_callback = WordSimilarityCallback(test_word=\"araba\", \n",
    "                                         word_to_ix=word_to_ix, \n",
    "                                         ix_to_word=ix_to_word,\n",
    "                                         top_k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1056ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print model architecture\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66ef5a14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start Training\n",
    "print(\"\\nStarting Training...\")\n",
    "history = model.fit(dataset, epochs=NUM_EPOCHS, callbacks=[visual_callback])\n",
    "print(\"Training Complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfb2cc6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get weights from the embedding layer\n",
    "# The shape will be (VOCAB_SIZE, EMBEDDING_DIM)\n",
    "vectors = model.get_layer(\"embedding_layer\").get_weights()[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ea43e86",
   "metadata": {},
   "source": [
    "#### Torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31ac3aed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec2f7364",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8585e34d",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(f'Using device: {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecc77cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Skip-gram Model\n",
    "class SkipGramModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super(SkipGramModel, self).__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.linear = nn.Linear(embedding_dim, vocab_size)\n",
    "\n",
    "    def forward(self, target_word_idx):\n",
    "        # target_word_idx shape: [batch_size]\n",
    "        embed = self.embeddings(target_word_idx) # shape: [batch_size, embedding_dim]\n",
    "        output = self.linear(embed)              # shape: [batch_size, vocab_size]\n",
    "        log_probs = torch.log_softmax(output, dim=1)\n",
    "        return log_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6e7efd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transforming into torch tensors for compatibility with data loaders which will be implementing batching also\n",
    "inputs_tensor = torch.tensor(inputs, dtype=torch.long)\n",
    "targets_tensor = torch.tensor(targets, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95322bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining data loader for batching\n",
    "train_data = TensorDataset(inputs_tensor, targets_tensor)\n",
    "train_loader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "143bd369",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize models\n",
    "skipgram_model = SkipGramModel(VOCAB_SIZE, EMBEDDING_DIM).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbb5644b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define loss function and optimizer\n",
    "criterion = nn.NLLLoss()\n",
    "skipgram_optimizer = optim.SGD(skipgram_model.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7bcf625",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training Skip-gram model (Conceptual)...\")\n",
    "for epoch in tqdm(range(NUM_EPOCHS), desc=\"Epochs\", position=0, leave=True):\n",
    "    total_loss = 0\n",
    "    \n",
    "    # DataLoader fetches batch by batch\n",
    "    for batch_inputs, batch_targets in tqdm(train_loader, desc=f\"Epoch {epoch+1} Batches\", leave=False):\n",
    "        \n",
    "        # Place data on GPU\n",
    "        batch_inputs = batch_inputs.to(device)\n",
    "        batch_targets = batch_targets.to(device)\n",
    "        \n",
    "        # Zeroing grads\n",
    "        skipgram_optimizer.zero_grad()\n",
    "        \n",
    "        # forward prop\n",
    "        log_probs = skipgram_model(batch_inputs)\n",
    "        \n",
    "        # loss calculation\n",
    "        loss = criterion(log_probs, batch_targets)\n",
    "        \n",
    "        # backward prop\n",
    "        loss.backward()\n",
    "        skipgram_optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{NUM_EPOCHS}, Loss: {total_loss/len(train_loader):.4f}\")\n",
    "print(\"Skip-gram training complete (Conceptual).\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
