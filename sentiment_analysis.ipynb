{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f1e8c077",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Dev\\Repos\\turkish-sentiment-classification\\.venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2c5c5ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.float_format', lambda x: '%.3f' % x)\n",
    "pd.set_option('display.width', 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d8407fab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_df(dataframe):\n",
    "    \"\"\"\n",
    "    Checks the overall structure and key metrics of a DataFrame.\n",
    "\n",
    "    Args:\n",
    "        dataframe (pd.DataFrame): DataFrame to inspect.\n",
    "\n",
    "    Returns:\n",
    "        None: Prints shape, data types, head, tail, missing values, and quantiles.\n",
    "    \"\"\"\n",
    "    print(\"##################### Shape #####################\")\n",
    "    print(dataframe.shape)\n",
    "    print(\"##################### Types #####################\")\n",
    "    print(dataframe.dtypes)\n",
    "    print(\"##################### Head #####################\")\n",
    "    print(dataframe.head(5))\n",
    "    print(\"##################### Tail #####################\")\n",
    "    print(dataframe.tail(5))\n",
    "    print(\"##################### NA #####################\")\n",
    "    print(dataframe.isnull().sum())\n",
    "    print('##################### Unique Values #####################')\n",
    "    print(dataframe.nunique())\n",
    "    print(\"##################### Duplicates #####################\")\n",
    "    print(dataframe.duplicated().sum())\n",
    "    print(\"##################### Quantiles #####################\")\n",
    "    # Uncomment below to include quantile information\n",
    "    #print(dataframe[[col for col in dataframe.columns if dataframe[col].dtypes != \"O\"]].quantile([0, 0.05, 0.50, 0.75, 0.95, 0.99, 1]).T)\n",
    "    print(dataframe.describe().T)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "70be243e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_train():\n",
    "    df_train = pd.read_csv(\"data/train.csv\", encoding=\"UTF-8\", engine=\"python\", encoding_errors=\"replace\")#replaces damaged bytes with \"\\ufffd\"\n",
    "    return df_train\n",
    "\n",
    "def load_test():\n",
    "    df_test = pd.read_csv(\"data/test.csv\", encoding=\"UTF-8\", engine=\"python\", encoding_errors=\"replace\")\n",
    "    return df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e15f244c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = load_train()\n",
    "df_test = load_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fda9fad3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##################### Shape #####################\n",
      "(440679, 2)\n",
      "##################### Types #####################\n",
      "text     object\n",
      "label    object\n",
      "dtype: object\n",
      "##################### Head #####################\n",
      "                                                text     label\n",
      "0  ürünü hepsiburadadan alalı 3 hafta oldu. orjin...  Positive\n",
      "1  ürünlerden çok memnunum, kesinlikle herkese ta...  Positive\n",
      "2      hızlı kargo, temiz alışveriş.teşekkür ederim.  Positive\n",
      "3               Çünkü aranan tapınak bu bölgededir .      Notr\n",
      "4  bu telefonu başlıca alma nedenlerim ise elimde...  Positive\n",
      "##################### Tail #####################\n",
      "                                                     text     label\n",
      "440674  Ayrıca burç yorumları ve çapraz bulmaca da der...      Notr\n",
      "440675  günümüz de ssd olmazsa olmaz bir donanım artık...  Positive\n",
      "440676  kullandım ve çok memnun kaldım. ocak başında d...  Positive\n",
      "440677                Adını Lenkeran şehrinden almıştır .      Notr\n",
      "440678  Bu dergilerde sosyalist teori ve siyasete iliş...      Notr\n",
      "##################### NA #####################\n",
      "text     0\n",
      "label    0\n",
      "dtype: int64\n",
      "##################### Unique Values #####################\n",
      "text     438997\n",
      "label         3\n",
      "dtype: int64\n",
      "##################### Duplicates #####################\n",
      "1616\n",
      "##################### Quantiles #####################\n",
      "        count  unique       top    freq\n",
      "text   440679  438997    #NAME?    1062\n",
      "label  440679       3  Positive  235949\n"
     ]
    }
   ],
   "source": [
    "check_df(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b783f213",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##################### Shape #####################\n",
      "(48965, 2)\n",
      "##################### Types #####################\n",
      "text     object\n",
      "label    object\n",
      "dtype: object\n",
      "##################### Head #####################\n",
      "                                                text     label\n",
      "0      Kral akbaba dikkat çekici renklere sahiptir .      Notr\n",
      "1   ısrarla korkutmayı başarıyor. sanki korku çok...  Positive\n",
      "2  Neşe ve Üzüntü köprünün kırılmaya başlamasıyla...      Notr\n",
      "3  i phone 5 ten sonra gene 4'' ekranı tercih ett...  Positive\n",
      "4    Beşinci sezonda diziye yeni oyuncular katıldı .      Notr\n",
      "##################### Tail #####################\n",
      "                                                    text     label\n",
      "48960  Fransa bayrağı diğer kırmızı zeminden beyaz bi...      Notr\n",
      "48961  Yine aynı yıl türkü dalında Murat Çobanoğlu il...      Notr\n",
      "48962                           Kurgunu skiyim oç evladı  Negative\n",
      "48963  Şarkı daha sonrasında Damian Marley tarafından...      Notr\n",
      "48964  berrak bir ürün ancak kendi orijinal spigen ly...  Negative\n",
      "##################### NA #####################\n",
      "text     0\n",
      "label    0\n",
      "dtype: int64\n",
      "##################### Unique Values #####################\n",
      "text     48830\n",
      "label        3\n",
      "dtype: int64\n",
      "##################### Duplicates #####################\n",
      "131\n",
      "##################### Quantiles #####################\n",
      "       count unique       top   freq\n",
      "text   48965  48830    #NAME?    119\n",
      "label  48965      3  Positive  26217\n"
     ]
    }
   ],
   "source": [
    "check_df(df_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12376162",
   "metadata": {},
   "source": [
    "- damaged rows filtering, these can be considered to be dropped\n",
    "- also other (#NAME?) damage can be seen during data read, these rows will be dropped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5a1f10f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total damaged rows in train: 7\n",
      "                                                     text     label\n",
      "31512   - su akıtmıyor: adamlar kullanam klavuzuna yaz...  Positive\n",
      "55634   -kargocu arkadaşlar ürünü bir bayan olarak taş...  Positive\n",
      "64093   - kullanım tarifindeki 'hazneye sıcak su koyun...  Positive\n",
      "102817  -kamerasına laf edilmiş. çıktığı dönemin en iy...  Positive\n",
      "332479  - karşı taraf sesimden çok memnun ama ben karş...  Positive\n",
      "Total damaged rows in test: 0\n",
      "Empty DataFrame\n",
      "Columns: [text, label]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "damaged_rows_train = df_train[df_train[\"text\"].str.contains(\"\\ufffd\", na=False)]\n",
    "damaged_rows_test = df_test[df_test[\"text\"].str.contains(\"\\ufffd\", na=False)]\n",
    "\n",
    "print(f\"Total damaged rows in train: {len(damaged_rows_train)}\")\n",
    "\n",
    "print(damaged_rows_train.head())\n",
    "\n",
    "print(f\"Total damaged rows in test: {len(damaged_rows_test)}\")\n",
    "\n",
    "print(damaged_rows_test.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1031daf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.drop(index=damaged_rows_train.index, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7ee79b63",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_train[df_train['text'] != \"#NAME?\"]\n",
    "df_test = df_test[df_test['text'] != \"#NAME?\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "548c2712",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in df_train.columns:\n",
    "    df_train[col] = df_train[col].str.lower() # Normalizing Case Folding\n",
    "    df_train[col] = df_train[col].str.replace(r'[^\\w\\s]', '', regex=True) # Punctuations\n",
    "    df_train[col] = df_train[col].str.replace(r'\\d+', '', regex=True) # Numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2f6e791b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in df_test.columns:\n",
    "    df_test[col] = df_test[col].str.lower() # Normalizing Case Folding\n",
    "    df_test[col] = df_test[col].str.replace(r'[^\\w\\s]', '', regex=True) # Punctuations\n",
    "    df_test[col] = df_test[col].str.replace(r'\\d+', '', regex=True) # Numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cfd5647e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##################### Shape #####################\n",
      "(439610, 2)\n",
      "##################### Types #####################\n",
      "text     object\n",
      "label    object\n",
      "dtype: object\n",
      "##################### Head #####################\n",
      "                                                text     label\n",
      "0  ürünü hepsiburadadan alalı  hafta oldu orjinal...  positive\n",
      "1  ürünlerden çok memnunum kesinlikle herkese tav...  positive\n",
      "2         hızlı kargo temiz alışverişteşekkür ederim  positive\n",
      "3                çünkü aranan tapınak bu bölgededir       notr\n",
      "4  bu telefonu başlıca alma nedenlerim ise elimde...  positive\n",
      "##################### Tail #####################\n",
      "                                                     text     label\n",
      "440674  ayrıca burç yorumları ve çapraz bulmaca da der...      notr\n",
      "440675  günümüz de ssd olmazsa olmaz bir donanım artık...  positive\n",
      "440676  kullandım ve çok memnun kaldım ocak başında da...  positive\n",
      "440677                 adını lenkeran şehrinden almıştır       notr\n",
      "440678  bu dergilerde sosyalist teori ve siyasete iliş...      notr\n",
      "##################### NA #####################\n",
      "text     0\n",
      "label    0\n",
      "dtype: int64\n",
      "##################### Unique Values #####################\n",
      "text     436516\n",
      "label         3\n",
      "dtype: int64\n",
      "##################### Duplicates #####################\n",
      "2999\n",
      "##################### Quantiles #####################\n",
      "        count  unique       top    freq\n",
      "text   439610  436516                58\n",
      "label  439610       3  positive  234994\n"
     ]
    }
   ],
   "source": [
    "check_df(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "836ee396",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##################### Shape #####################\n",
      "(48846, 2)\n",
      "##################### Types #####################\n",
      "text     object\n",
      "label    object\n",
      "dtype: object\n",
      "##################### Head #####################\n",
      "                                                text     label\n",
      "0       kral akbaba dikkat çekici renklere sahiptir       notr\n",
      "1   ısrarla korkutmayı başarıyor sanki korku çok ...  positive\n",
      "2  neşe ve üzüntü köprünün kırılmaya başlamasıyla...      notr\n",
      "3  i phone  ten sonra gene  ekranı tercih ettim t...  positive\n",
      "4     beşinci sezonda diziye yeni oyuncular katıldı       notr\n",
      "##################### Tail #####################\n",
      "                                                    text     label\n",
      "48960  fransa bayrağı diğer kırmızı zeminden beyaz bi...      notr\n",
      "48961  yine aynı yıl türkü dalında murat çobanoğlu il...      notr\n",
      "48962                           kurgunu skiyim oç evladı  negative\n",
      "48963  şarkı daha sonrasında damian marley tarafından...      notr\n",
      "48964  berrak bir ürün ancak kendi orijinal spigen ly...  negative\n",
      "##################### NA #####################\n",
      "text     0\n",
      "label    0\n",
      "dtype: int64\n",
      "##################### Unique Values #####################\n",
      "text     48768\n",
      "label        3\n",
      "dtype: int64\n",
      "##################### Duplicates #####################\n",
      "70\n",
      "##################### Quantiles #####################\n",
      "       count unique       top   freq\n",
      "text   48846  48768                9\n",
      "label  48846      3  positive  26110\n"
     ]
    }
   ],
   "source": [
    "check_df(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "602703ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.drop_duplicates(inplace=True)\n",
    "df_test.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "432d409c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##################### Shape #####################\n",
      "(436611, 2)\n",
      "##################### Types #####################\n",
      "text     object\n",
      "label    object\n",
      "dtype: object\n",
      "##################### Head #####################\n",
      "                                                text     label\n",
      "0  ürünü hepsiburadadan alalı  hafta oldu orjinal...  positive\n",
      "1  ürünlerden çok memnunum kesinlikle herkese tav...  positive\n",
      "2         hızlı kargo temiz alışverişteşekkür ederim  positive\n",
      "3                çünkü aranan tapınak bu bölgededir       notr\n",
      "4  bu telefonu başlıca alma nedenlerim ise elimde...  positive\n",
      "##################### Tail #####################\n",
      "                                                     text     label\n",
      "440674  ayrıca burç yorumları ve çapraz bulmaca da der...      notr\n",
      "440675  günümüz de ssd olmazsa olmaz bir donanım artık...  positive\n",
      "440676  kullandım ve çok memnun kaldım ocak başında da...  positive\n",
      "440677                 adını lenkeran şehrinden almıştır       notr\n",
      "440678  bu dergilerde sosyalist teori ve siyasete iliş...      notr\n",
      "##################### NA #####################\n",
      "text     0\n",
      "label    0\n",
      "dtype: int64\n",
      "##################### Unique Values #####################\n",
      "text     436516\n",
      "label         3\n",
      "dtype: int64\n",
      "##################### Duplicates #####################\n",
      "0\n",
      "##################### Quantiles #####################\n",
      "        count  unique       top    freq\n",
      "text   436611  436516       mrb       3\n",
      "label  436611       3  positive  232626\n",
      "##################### Shape #####################\n",
      "(48776, 2)\n",
      "##################### Types #####################\n",
      "text     object\n",
      "label    object\n",
      "dtype: object\n",
      "##################### Head #####################\n",
      "                                                text     label\n",
      "0       kral akbaba dikkat çekici renklere sahiptir       notr\n",
      "1   ısrarla korkutmayı başarıyor sanki korku çok ...  positive\n",
      "2  neşe ve üzüntü köprünün kırılmaya başlamasıyla...      notr\n",
      "3  i phone  ten sonra gene  ekranı tercih ettim t...  positive\n",
      "4     beşinci sezonda diziye yeni oyuncular katıldı       notr\n",
      "##################### Tail #####################\n",
      "                                                    text     label\n",
      "48960  fransa bayrağı diğer kırmızı zeminden beyaz bi...      notr\n",
      "48961  yine aynı yıl türkü dalında murat çobanoğlu il...      notr\n",
      "48962                           kurgunu skiyim oç evladı  negative\n",
      "48963  şarkı daha sonrasında damian marley tarafından...      notr\n",
      "48964  berrak bir ürün ancak kendi orijinal spigen ly...  negative\n",
      "##################### NA #####################\n",
      "text     0\n",
      "label    0\n",
      "dtype: int64\n",
      "##################### Unique Values #####################\n",
      "text     48768\n",
      "label        3\n",
      "dtype: int64\n",
      "##################### Duplicates #####################\n",
      "0\n",
      "##################### Quantiles #####################\n",
      "       count unique       top   freq\n",
      "text   48776  48768                2\n",
      "label  48776      3  positive  26053\n"
     ]
    }
   ],
   "source": [
    "check_df(df_train)\n",
    "check_df(df_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faec8354",
   "metadata": {},
   "source": [
    "**TASK**\n",
    "\n",
    "4 different models ([TF-IDF with Multinomial Naive Bayes and Binary Naive Bayes] + [ANN with Word2Vec and FastText]) will be trained and compared."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86ff79b7",
   "metadata": {},
   "source": [
    "**ROADMAP**\n",
    "\n",
    "Preprocessing steps will be applied on data according to models they will be fed to.\n",
    "\n",
    "***For Bayesian Model:***\n",
    "- Lowecase transformation\n",
    "- Special characters cleaning (Punctuations etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "499d0ba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def concat_df_on_y_axis(df_1, df_2):\n",
    "    \"\"\"\n",
    "    Concatenates two DataFrames along the Y-axis (rows).\n",
    "\n",
    "    Args:\n",
    "        df_1 (pd.DataFrame): First DataFrame.\n",
    "        df_2 (pd.DataFrame): Second DataFrame.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Concatenated DataFrame.\n",
    "    \"\"\"\n",
    "    return pd.concat([df_1, df_2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "61a587fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_test = concat_df_on_y_axis(df_train, df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f1998d13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##################### Shape #####################\n",
      "(485387, 2)\n",
      "##################### Types #####################\n",
      "text     object\n",
      "label    object\n",
      "dtype: object\n",
      "##################### Head #####################\n",
      "                                                text     label\n",
      "0  ürünü hepsiburadadan alalı  hafta oldu orjinal...  positive\n",
      "1  ürünlerden çok memnunum kesinlikle herkese tav...  positive\n",
      "2         hızlı kargo temiz alışverişteşekkür ederim  positive\n",
      "3                çünkü aranan tapınak bu bölgededir       notr\n",
      "4  bu telefonu başlıca alma nedenlerim ise elimde...  positive\n",
      "##################### Tail #####################\n",
      "                                                    text     label\n",
      "48960  fransa bayrağı diğer kırmızı zeminden beyaz bi...      notr\n",
      "48961  yine aynı yıl türkü dalında murat çobanoğlu il...      notr\n",
      "48962                           kurgunu skiyim oç evladı  negative\n",
      "48963  şarkı daha sonrasında damian marley tarafından...      notr\n",
      "48964  berrak bir ürün ancak kendi orijinal spigen ly...  negative\n",
      "##################### NA #####################\n",
      "text     0\n",
      "label    0\n",
      "dtype: int64\n",
      "##################### Unique Values #####################\n",
      "text     484755\n",
      "label         3\n",
      "dtype: int64\n",
      "##################### Duplicates #####################\n",
      "515\n",
      "##################### Quantiles #####################\n",
      "        count  unique       top    freq\n",
      "text   485387  484755                 5\n",
      "label  485387       3  positive  258679\n"
     ]
    }
   ],
   "source": [
    "check_df(df_train_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c816f75",
   "metadata": {},
   "source": [
    "**OBSERVATIONS**\n",
    "- df_train has 0 duplicates, duplicates dropped.\n",
    "- df_test has 0 duplicates, duplicates dropped.\n",
    "- df_train_test has 515 duplicates.\n",
    "- **Data Leakage observed**\n",
    "- Set of {df_train INTERSECT df_test} has to be removed from df_train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0e5f0833",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_texts = set(df_test['text'])\n",
    "df_train = df_train[~df_train['text'].isin(test_texts)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "650dff71",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_test = concat_df_on_y_axis(df_train, df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a35c1f59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##################### Shape #####################\n",
      "(484835, 2)\n",
      "##################### Types #####################\n",
      "text     object\n",
      "label    object\n",
      "dtype: object\n",
      "##################### Head #####################\n",
      "                                                text     label\n",
      "0  ürünü hepsiburadadan alalı  hafta oldu orjinal...  positive\n",
      "1  ürünlerden çok memnunum kesinlikle herkese tav...  positive\n",
      "2         hızlı kargo temiz alışverişteşekkür ederim  positive\n",
      "3                çünkü aranan tapınak bu bölgededir       notr\n",
      "4  bu telefonu başlıca alma nedenlerim ise elimde...  positive\n",
      "##################### Tail #####################\n",
      "                                                    text     label\n",
      "48960  fransa bayrağı diğer kırmızı zeminden beyaz bi...      notr\n",
      "48961  yine aynı yıl türkü dalında murat çobanoğlu il...      notr\n",
      "48962                           kurgunu skiyim oç evladı  negative\n",
      "48963  şarkı daha sonrasında damian marley tarafından...      notr\n",
      "48964  berrak bir ürün ancak kendi orijinal spigen ly...  negative\n",
      "##################### NA #####################\n",
      "text     0\n",
      "label    0\n",
      "dtype: int64\n",
      "##################### Unique Values #####################\n",
      "text     484755\n",
      "label         3\n",
      "dtype: int64\n",
      "##################### Duplicates #####################\n",
      "0\n",
      "##################### Quantiles #####################\n",
      "        count  unique       top    freq\n",
      "text   484835  484755       mrb       3\n",
      "label  484835       3  positive  258277\n"
     ]
    }
   ],
   "source": [
    "check_df(df_train_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02c65be0",
   "metadata": {},
   "source": [
    "**Data Leakage problem solved**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31ca3654",
   "metadata": {},
   "source": [
    "## Naive Bayes Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34564fd0",
   "metadata": {},
   "source": [
    "**STOPWORDS REMOVAL**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bafd1330",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fe655ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('stopwords')\n",
    "\n",
    "sw = stopwords.words('turkish')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2adad87",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_test_sw_removed = df_train_test.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "670c917c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_test_sw_removed['text'] = df_train_test_sw_removed['text'].apply(lambda x: \" \".join(x for x in str(x).split() if x not in sw))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ab6920c",
   "metadata": {},
   "source": [
    "#### Stemming\n",
    "- Stemming is easy and will produce enough efficiency with bayesian models\n",
    "- Lemmatization can be alternative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf3417d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from TurkishStemmer import TurkishStemmer\n",
    "stemmer = TurkishStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78b1e10d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_test_sw_removed['text'] = df_train_test_sw_removed['text'].apply(lambda x: \" \".join([stemmer.stem(word) for word in x.split()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3b860ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "check_df(df_train_test_sw_removed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69a1f00b",
   "metadata": {},
   "outputs": [],
   "source": [
    "len_train = len(df_train)\n",
    "\n",
    "df_train_sw_removed_stemmed = df_train_test_sw_removed.iloc[:len_train].copy()\n",
    "\n",
    "df_test_sw_removed_stemmed = df_train_test_sw_removed.iloc[len_train:].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d88a0f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = df_train_sw_removed_stemmed['text']\n",
    "y_train = df_train_sw_removed_stemmed['label']\n",
    "X_test = df_test_sw_removed_stemmed['text']\n",
    "y_test = df_test_sw_removed_stemmed['label']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5644704b",
   "metadata": {},
   "source": [
    "#### Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b713aa55",
   "metadata": {},
   "outputs": [],
   "source": [
    "import stanza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "65f99b52",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.11.0.json: 435kB [00:00, 6.72MB/s]                    \n",
      "2025-12-21 19:25:00 INFO: Downloaded file to C:\\Users\\borab\\stanza_resources\\resources.json\n",
      "2025-12-21 19:25:00 INFO: Downloading default packages for language: tr (Turkish) ...\n",
      "2025-12-21 19:25:00 INFO: File exists: C:\\Users\\borab\\stanza_resources\\tr\\default.zip\n",
      "2025-12-21 19:25:02 INFO: Finished downloading models and saved to C:\\Users\\borab\\stanza_resources\n"
     ]
    }
   ],
   "source": [
    "# Download tr model\n",
    "stanza.download('tr') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8f4acf44",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-21 19:25:06 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n",
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.11.0.json: 435kB [00:00, 7.81MB/s]                    \n",
      "2025-12-21 19:25:06 INFO: Downloaded file to C:\\Users\\borab\\stanza_resources\\resources.json\n",
      "2025-12-21 19:25:06 WARNING: Language tr package default expects mwt, which has been added\n",
      "2025-12-21 19:25:06 INFO: Loading these models for language: tr (Turkish):\n",
      "=============================\n",
      "| Processor | Package       |\n",
      "-----------------------------\n",
      "| tokenize  | imst          |\n",
      "| mwt       | imst          |\n",
      "| lemma     | imst_nocharlm |\n",
      "=============================\n",
      "\n",
      "2025-12-21 19:25:06 INFO: Using device: cuda\n",
      "2025-12-21 19:25:06 INFO: Loading: tokenize\n",
      "2025-12-21 19:25:07 INFO: Loading: mwt\n",
      "2025-12-21 19:25:07 INFO: Loading: lemma\n",
      "2025-12-21 19:25:07 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "# Pipeline (use_gpu=True for gpu usage)\n",
    "nlp = stanza.Pipeline('tr', processors='tokenize,lemma', use_gpu=True)\n",
    "\n",
    "def stanza_lemmatizer(text):\n",
    "    doc = nlp(text)\n",
    "    lemmas = []\n",
    "    # Stanza splits documents into sentences and words\n",
    "    for sentence in doc.sentences:\n",
    "        for word in sentence.words:\n",
    "            if word.lemma is not None:\n",
    "                lemmas.append(word.lemma)\n",
    "            else:\n",
    "                lemmas.append(word.text)\n",
    "    return \" \".join(lemmas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "28a34954",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_test_lemmatized = df_train_test.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "db80bf31",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = df_train_test_lemmatized['text'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2b622be8",
   "metadata": {},
   "outputs": [],
   "source": [
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c15ffc2",
   "metadata": {},
   "source": [
    "**STANZA FOR LEMMATIZATION**\n",
    "- In order to perform Lemmatization contextually, stanza library is used.\n",
    "- stanza utilizes cuda and gpu technology to accelerate computation.\n",
    "- Still, pandas methods creates bottleneck so we say hi to our old friend, batching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d7a6e1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 512 \n",
    "\n",
    "processed_results = []\n",
    "\n",
    "print(f\"Total {len(texts)} row, {BATCH_SIZE}: batch size\")\n",
    "\n",
    "for i in tqdm(range(0, len(texts), BATCH_SIZE), desc=\"GPU is processing\"):\n",
    "    batch_texts = texts[i : i + BATCH_SIZE]\n",
    "    \n",
    "    in_docs = [stanza.Document([], text=d) for d in batch_texts]\n",
    "    \n",
    "    out_docs = nlp(in_docs)\n",
    "    \n",
    "    for doc in out_docs:\n",
    "        lemmas = []\n",
    "        for sentence in doc.sentences:\n",
    "            for word in sentence.words:\n",
    "                if word.lemma:\n",
    "                    lemmas.append(word.lemma)\n",
    "                else:\n",
    "                    lemmas.append(word.text)\n",
    "        \n",
    "        processed_results.append(\" \".join(lemmas))\n",
    "\n",
    "df_train_test_lemmatized['text'] = processed_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c94ac083",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_test_lemmatized.to_csv(\"data/df_train_test_prep.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86b5bf27",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_train_test_lemmatized['text'] = df_train_test_lemmatized['text'].progress_apply(lambda x: stanza_lemmatizer(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c97e97e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "len_train = len(df_train)\n",
    "\n",
    "df_train_lemmatized = df_train_test_lemmatized.iloc[:len_train].copy()\n",
    "\n",
    "df_test_lemmatized = df_train_test_lemmatized.iloc[len_train:].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1c4efe4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = df_train_lemmatized['text']\n",
    "y_train = df_train_lemmatized['label']\n",
    "X_test = df_test_lemmatized['text']\n",
    "y_test = df_test_lemmatized['label']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f185b8a",
   "metadata": {},
   "source": [
    "### Multinomial Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "869d97c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9a8fe91b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB, BernoulliNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "fc2d1144",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "548e3002",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(ngram_range=(1,2))# unigram and bigrams"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03045a86",
   "metadata": {},
   "source": [
    "**TF-IDF Vectorization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7eb60f61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for multinomial NB\n",
    "X_train_nb = tfidf_vectorizer.fit_transform(X_train)\n",
    "X_test_nb = tfidf_vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be7de2b3",
   "metadata": {},
   "source": [
    "**Multinomial NB Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ab9f2234",
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_model = MultinomialNB().fit(X_train_nb, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88576a58",
   "metadata": {},
   "source": [
    "**Multinomial NB Model Evaluation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e51c22f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_model_pred = nb_model.predict(X_test_nb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "292f4601",
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_model_pred_train = nb_model.predict(X_train_nb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "edbb23d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative      0.996     0.120     0.214      5636\n",
      "        notr      0.988     0.918     0.952     17087\n",
      "    positive      0.804     0.995     0.889     26053\n",
      "\n",
      "    accuracy                          0.867     48776\n",
      "   macro avg      0.929     0.677     0.685     48776\n",
      "weighted avg      0.891     0.867     0.833     48776\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, nb_model_pred, digits=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ae615c1",
   "metadata": {},
   "source": [
    "### Binary Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9176f7c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "17eb2275",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB, BernoulliNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6069a9cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a3017372",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer_binary = TfidfVectorizer(ngram_range=(1,2), binary=True)#unigrams and bigrams"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26e57425",
   "metadata": {},
   "source": [
    "**Binary TF-IDF Vectorization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e8affbe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for binary NB\n",
    "X_train_nb_binary = tfidf_vectorizer_binary.fit_transform(X_train)\n",
    "X_test_nb_binary = tfidf_vectorizer_binary.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a07c2fa",
   "metadata": {},
   "source": [
    "**Binary NB Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "6b0e70f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_model_binary = BernoulliNB().fit(X_train_nb_binary, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "295ab00d",
   "metadata": {},
   "source": [
    "**Binary NB Model Evaluation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "13684c0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_binary_model_pred = nb_model.predict(X_test_nb_binary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "6ad09e17",
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_binary_model_pred_train = nb_model.predict(X_train_nb_binary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "fc1f8187",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative      0.996     0.123     0.219      5636\n",
      "        notr      0.988     0.919     0.952     17087\n",
      "    positive      0.805     0.995     0.890     26053\n",
      "\n",
      "    accuracy                          0.867     48776\n",
      "   macro avg      0.930     0.679     0.687     48776\n",
      "weighted avg      0.891     0.867     0.834     48776\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, nb_binary_model_pred, digits=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "f2f4509a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score\n",
    "\n",
    "def get_classification_report(y_true, y_pred, algorithm_name=\"Algorithm Model Name\"):\n",
    "\n",
    "    target_labels = ['positive', 'negative', 'notr'] \n",
    "    \n",
    "    # A) Micro Average\n",
    "    # In multi-class classification, Micro Precision = Micro Recall = Micro F1 = Accuracy\n",
    "    micro_p, micro_r, micro_f, _ = precision_recall_fscore_support(\n",
    "        y_true, y_pred, average='micro'\n",
    "    )\n",
    "    \n",
    "    # B) Macro Average\n",
    "    macro_p, macro_r, macro_f, _ = precision_recall_fscore_support(\n",
    "        y_true, y_pred, average='macro'\n",
    "    )\n",
    "    \n",
    "    # C) Class-Specific Metrics (Positive, Negative, Notr)\n",
    "    # average=None returns the scores for each class in the order of 'labels'\n",
    "    class_p, class_r, class_f, _ = precision_recall_fscore_support(\n",
    "        y_true, y_pred, average=None, labels=target_labels\n",
    "    )\n",
    "    \n",
    "    # We create a dictionary to hold the data\n",
    "    report_data = {\n",
    "        \"Metric\": [\n",
    "            \"Micro Average Recall\", \"Micro Average Precision\", \"Micro Average F-Score\",\n",
    "            \"Macro Average Recall\", \"Macro Average Precision\", \"Macro Average F-Score\",\n",
    "            \"Class: Positive Recall\", \"Class: Positive Precision\", \"Class: Positive F-Score\",\n",
    "            \"Class: Negative Recall\", \"Class: Negative Precision\", \"Class: Negative F-Score\",\n",
    "            \"Class: Notr Recall\", \"Class: Notr Precision\", \"Class: Notr F-Score\"\n",
    "        ],\n",
    "        algorithm_name: [\n",
    "            # Micro\n",
    "            f\"{micro_r:.3f}\", f\"{micro_p:.3f}\", f\"{micro_f:.3f}\",\n",
    "            # Macro\n",
    "            f\"{macro_r:.3f}\", f\"{macro_p:.3f}\", f\"{macro_f:.3f}\",\n",
    "            # Positive (Index 0 in target_labels)\n",
    "            f\"{class_r[0]:.3f}\", f\"{class_p[0]:.3f}\", f\"{class_f[0]:.3f}\",\n",
    "            # Negative (Index 1 in target_labels)\n",
    "            f\"{class_r[1]:.3f}\", f\"{class_p[1]:.3f}\", f\"{class_f[1]:.3f}\",\n",
    "            # Notr (Index 2 in target_labels)\n",
    "            f\"{class_r[2]:.3f}\", f\"{class_p[2]:.3f}\", f\"{class_f[2]:.3f}\"\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    # Create DataFrame and set metric as the index\n",
    "    df_report = pd.DataFrame(report_data)\n",
    "    df_report = df_report.set_index(\"Metric\")\n",
    "    \n",
    "    return df_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "6490e129",
   "metadata": {},
   "outputs": [],
   "source": [
    "cls_rep_MNB = get_classification_report(y_test, nb_model_pred, \"Multinomial Naive Bayes\")\n",
    "cls_rep_BNB = get_classification_report(y_test, nb_binary_model_pred, \"Binary Naive Bayes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "2d588af6",
   "metadata": {},
   "outputs": [],
   "source": [
    "cls_rep_MNB_train = get_classification_report(y_train, nb_model_pred_train, \"Multinomial Naive Bayes\")\n",
    "cls_rep_BNB_train = get_classification_report(y_train, nb_binary_model_pred_train, \"Binary Naive Bayes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da126f54",
   "metadata": {},
   "source": [
    "**TEST**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "28916445",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                          Multinomial Naive Bayes Binary Naive Bayes\n",
      "Metric                                                              \n",
      "Micro Average Recall                        0.867              0.867\n",
      "Micro Average Precision                     0.867              0.867\n",
      "Micro Average F-Score                       0.867              0.867\n",
      "Macro Average Recall                        0.677              0.679\n",
      "Macro Average Precision                     0.929              0.930\n",
      "Macro Average F-Score                       0.685              0.687\n",
      "Class: Positive Recall                      0.995              0.995\n",
      "Class: Positive Precision                   0.804              0.805\n",
      "Class: Positive F-Score                     0.889              0.890\n",
      "Class: Negative Recall                      0.120              0.123\n",
      "Class: Negative Precision                   0.996              0.996\n",
      "Class: Negative F-Score                     0.214              0.219\n",
      "Class: Notr Recall                          0.918              0.919\n",
      "Class: Notr Precision                       0.988              0.988\n",
      "Class: Notr F-Score                         0.952              0.952\n"
     ]
    }
   ],
   "source": [
    "final_table_test = pd.concat([cls_rep_MNB, cls_rep_BNB], axis=1)\n",
    "print(final_table_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "344b2807",
   "metadata": {},
   "source": [
    "**TRAIN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "d33e08b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                          Multinomial Naive Bayes Binary Naive Bayes\n",
      "Metric                                                              \n",
      "Micro Average Recall                        0.898              0.900\n",
      "Micro Average Precision                     0.898              0.900\n",
      "Micro Average F-Score                       0.898              0.900\n",
      "Macro Average Recall                        0.735              0.740\n",
      "Macro Average Precision                     0.945              0.945\n",
      "Macro Average F-Score                       0.761              0.767\n",
      "Class: Positive Recall                      0.997              0.997\n",
      "Class: Positive Precision                   0.843              0.845\n",
      "Class: Positive F-Score                     0.913              0.915\n",
      "Class: Negative Recall                      0.243              0.257\n",
      "Class: Negative Precision                   0.999              0.999\n",
      "Class: Negative F-Score                     0.391              0.409\n",
      "Class: Notr Recall                          0.964              0.964\n",
      "Class: Notr Precision                       0.993              0.993\n",
      "Class: Notr F-Score                         0.978              0.978\n"
     ]
    }
   ],
   "source": [
    "final_table_train = pd.concat([cls_rep_MNB_train, cls_rep_BNB_train], axis=1)\n",
    "print(final_table_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fd57429",
   "metadata": {},
   "source": [
    "## ANN MODELİNG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de2ca016",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('punkt_tab') #necessary for tokenization method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c301254b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizing sentences\n",
    "tokenized_sentences_train = [word_tokenize(sentence.lower()) for sentence in df_train['text']]\n",
    "tokenized_sentences_test = [word_tokenize(sentence.lower()) for sentence in df_test['text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c07448fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "# Save training tokens\n",
    "with open('data/tokenized_train.pkl', 'wb') as f: # 'wb' stands for write binary\n",
    "    pickle.dump(tokenized_sentences_train, f)\n",
    "\n",
    "# Save test tokens\n",
    "with open('data/tokenized_test.pkl', 'wb') as f:\n",
    "    pickle.dump(tokenized_sentences_test, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb29719f",
   "metadata": {},
   "source": [
    "### SKIPGRAM-ANN MODEL\n",
    "- Skipgram has better performance modeling semantics, which we desperately need in this case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6459c0a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ee7daec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "EMBEDDING_DIM = 150      # Increased from 5 to 100 for better representation\n",
    "WINDOW_SIZE = 2\n",
    "MAX_VOCAB_SIZE = 20000   # Limit vocabulary to top 20k words to prevent OOM errors\n",
    "BATCH_SIZE = 1024         \n",
    "NUM_EPOCHS = 10       \n",
    "LEARNING_RATE = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02312676",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = tokenized_sentences_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d7ae975",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten the list of sentences to a single list of words\n",
    "all_words = [word for sentence in sentences for word in sentence]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54e4cb9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select only the most common words to keep the vocabulary size manageable\n",
    "# We reserve index 0 for <UNK>, so we take MAX_VOCAB_SIZE - 1\n",
    "word_counts = Counter(all_words).most_common(MAX_VOCAB_SIZE - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d9f980d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create vocabulary mapping: <UNK> is always at index 0\n",
    "word_to_ix = {\"<UNK>\": 0}\n",
    "for word, count in word_counts:\n",
    "    word_to_ix[word] = len(word_to_ix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c5035b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create reverse mapping (Index -> Word)\n",
    "ix_to_word = {ix: word for word, ix in word_to_ix.items()}\n",
    "VOCAB_SIZE = len(word_to_ix)\n",
    "\n",
    "print(f\"Total words scanned: {len(all_words)}\")\n",
    "print(f\"Final Vocabulary Size: {VOCAB_SIZE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d478d88e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Skip-gram Pairs (Input -> Target)\n",
    "inputs = []\n",
    "targets = []\n",
    "\n",
    "print(\"Generating training pairs...\")\n",
    "for sentence in sentences:\n",
    "    # Convert words to indices. If a word is not in top 20k, it becomes 0 (<UNK>)\n",
    "    sentence_indices = [word_to_ix.get(word, 0) for word in sentence]\n",
    "    \n",
    "    for i in range(len(sentence_indices)):\n",
    "        target_word_idx = sentence_indices[i] # Center word\n",
    "        \n",
    "        # Optimization: If the target word is unknown (<UNK>), \n",
    "        # we skip training on it to avoid noise.\n",
    "        if target_word_idx == 0:\n",
    "            continue\n",
    "            \n",
    "        # Define context window\n",
    "        start_idx = max(0, i - WINDOW_SIZE)\n",
    "        end_idx = min(len(sentence_indices), i + WINDOW_SIZE + 1)\n",
    "        \n",
    "        for j in range(start_idx, end_idx):\n",
    "            if i != j: # Skip the target word itself\n",
    "                context_word_idx = sentence_indices[j]\n",
    "                inputs.append(target_word_idx)\n",
    "                targets.append(context_word_idx)\n",
    "\n",
    "print(f\"Total training pairs generated: {len(inputs)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9655fcfa",
   "metadata": {},
   "source": [
    "#### Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71aa8217",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import site\n",
    "\n",
    "try:\n",
    "    site_packages = site.getsitepackages()[0]\n",
    "    nvidia_path = os.path.join(site_packages, 'nvidia')\n",
    "    \n",
    "    cudnn_path = os.path.join(nvidia_path, 'cudnn', 'lib')\n",
    "    cuda_path = os.path.join(nvidia_path, 'cuda_runtime', 'lib')\n",
    "    \n",
    "    old_ld = os.environ.get('LD_LIBRARY_PATH', '')\n",
    "    os.environ['LD_LIBRARY_PATH'] = f\"{cudnn_path}:{cuda_path}:{old_ld}\"\n",
    "    \n",
    "    # This specific flag often fixes 'DNN library initialization failed' errors\n",
    "    # by disabling some auto-tuning features that might crash on certain GPUs.\n",
    "    os.environ['TF_CUDNN_USE_AUTOTUNE'] = '0' \n",
    "    \n",
    "    print(\"NVIDIA Library paths arranged successfully\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Path warning: {e}\")\n",
    "\n",
    "# --- 2. IMPORT TENSORFLOW AND CONFIGURE GPU MEMORY ---\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.keras import layers, models, optimizers, callbacks\n",
    "\n",
    "print(f\"TensorFlow Version: {tf.__version__}\")\n",
    "\n",
    "# GPU Memory Growth\n",
    "# This is CRITICAL. It prevents TensorFlow from hogging all VRAM at start-up.\n",
    "# Must be run immediately after importing TF.\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        print(f\"GPU Detected and memory growth set: {gpus}\")\n",
    "    except RuntimeError as e:\n",
    "        print(f\"GPU Error: {e}\")\n",
    "else:\n",
    "    print(\"No GPU detected.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a8b5228",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert lists to NumPy arrays (TensorFlow prefers typed arrays)\n",
    "inputs = np.array(inputs, dtype=np.int32)\n",
    "targets = np.array(targets, dtype=np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4ee96aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use tf.data.Dataset for efficient Batching and Prefetching on GPU\n",
    "dataset = tf.data.Dataset.from_tensor_slices((inputs, targets))\n",
    "\n",
    "# Shuffle buffer size should ideally be >= number of training samples\n",
    "# Prefetch allows the CPU to prepare the next batch while GPU processes the current one\n",
    "dataset = dataset.shuffle(buffer_size=1024).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c386bf9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_skipgram_model(vocab_size, embedding_dim):\n",
    "    # Input layer: Receives a single integer (word index)\n",
    "    input_word = layers.Input(shape=(1,), name=\"target_word_input\")\n",
    "    \n",
    "    # Embedding layer: Converts index to dense vector\n",
    "    # input_dim: Vocabulary size\n",
    "    # output_dim: Size of the vector space\n",
    "    x = layers.Embedding(input_dim=vocab_size, \n",
    "                         output_dim=embedding_dim, \n",
    "                         input_length=1, \n",
    "                         name=\"embedding_layer\")(input_word)\n",
    "    \n",
    "    # Flatten: Converts (Batch, 1, Dim) -> (Batch, Dim)\n",
    "    x = layers.Flatten()(x)\n",
    "    \n",
    "    # Output layer: Predicts probability for every word in vocabulary\n",
    "    # Softmax ensures output sums to 1 (probability distribution)\n",
    "    output = layers.Dense(vocab_size, activation='softmax', name=\"context_prediction\")(x)\n",
    "    \n",
    "    model = models.Model(inputs=input_word, outputs=output)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "703ddb03",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WordSimilarityCallback(callbacks.Callback):\n",
    "    \"\"\"\n",
    "    A custom callback to print the most similar words to a specific query word\n",
    "    at the end of each epoch. This helps in monitoring the semantic learning progress.\n",
    "    \"\"\"\n",
    "    def __init__(self, test_word, word_to_ix, ix_to_word, top_k=5):\n",
    "        super(WordSimilarityCallback, self).__init__()\n",
    "        self.test_word = test_word\n",
    "        self.word_to_ix = word_to_ix\n",
    "        self.ix_to_word = ix_to_word\n",
    "        self.top_k = top_k\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        # Check if the test word is in vocabulary\n",
    "        if self.test_word not in self.word_to_ix:\n",
    "            return\n",
    "\n",
    "        # 1. Retrieve the weights from the embedding layer\n",
    "        embedding_layer = self.model.get_layer(\"embedding_layer\")\n",
    "        embeddings = embedding_layer.get_weights()[0]\n",
    "        \n",
    "        # 2. Get the vector for the test word\n",
    "        test_idx = self.word_to_ix[self.test_word]\n",
    "        test_vector = embeddings[test_idx]\n",
    "        \n",
    "        # 3. Calculate Cosine Similarity\n",
    "        # Normalize embeddings and the test vector to unit length\n",
    "        norm_embeddings = tf.math.l2_normalize(embeddings, axis=1)\n",
    "        norm_test_vector = tf.math.l2_normalize(test_vector, axis=0)\n",
    "        \n",
    "        # Dot product of normalized vectors equals cosine similarity\n",
    "        cosine_similarities = tf.tensordot(norm_embeddings, norm_test_vector, axes=1)\n",
    "        \n",
    "        # 4. Find the indices of the words with the highest similarity scores\n",
    "        # We take top_k + 1 because the most similar word is the word itself (score=1.0)\n",
    "        top_indices = tf.math.top_k(cosine_similarities, k=self.top_k + 1).indices.numpy()\n",
    "        \n",
    "        # 5. Print the results\n",
    "        closest_words = [self.ix_to_word[idx] for idx in top_indices if idx != test_idx]\n",
    "        print(f\"\\n[Validation] End of Epoch {epoch+1} - Closest words to '{self.test_word}':\")\n",
    "        print(f\"  -> {', '.join(closest_words)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f1752eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model\n",
    "model = build_skipgram_model(VOCAB_SIZE, EMBEDDING_DIM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "112d130a",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_k_metric = tf.keras.metrics.SparseTopKCategoricalAccuracy(k=5, name='top_5_acc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d275c98d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We use 'sparse_categorical_crossentropy' because our targets are integers (indexes),\n",
    "# not one-hot encoded vectors. This saves memory and is computationally efficient.\n",
    "model.compile(optimizer=optimizers.Adam(learning_rate=LEARNING_RATE),\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=[top_k_metric])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f04fca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the custom callback (e.g., check neighbors of \"learning\")\n",
    "# Note: Ensure the test_word exists in your training data\n",
    "visual_callback = WordSimilarityCallback(test_word=\"araba\", \n",
    "                                         word_to_ix=word_to_ix, \n",
    "                                         ix_to_word=ix_to_word,\n",
    "                                         top_k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1056ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print model architecture\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66ef5a14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start Training\n",
    "print(\"\\nStarting Training...\")\n",
    "history = model.fit(dataset, epochs=NUM_EPOCHS, callbacks=[visual_callback])\n",
    "print(\"Training Complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfb2cc6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get weights from the embedding layer\n",
    "# The shape will be (VOCAB_SIZE, EMBEDDING_DIM)\n",
    "vectors = model.get_layer(\"embedding_layer\").get_weights()[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ea43e86",
   "metadata": {},
   "source": [
    "#### Torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31ac3aed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.nn.functional as F\n",
    "import json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec2f7364",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8585e34d",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(f'Using device: {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecc77cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Skip-gram Model\n",
    "class SkipGramModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super(SkipGramModel, self).__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.linear = nn.Linear(embedding_dim, vocab_size)\n",
    "\n",
    "    def forward(self, target_word_idx):\n",
    "        # target_word_idx shape: [batch_size]\n",
    "        embed = self.embeddings(target_word_idx) # shape: [batch_size, embedding_dim]\n",
    "        output = self.linear(embed)              # shape: [batch_size, vocab_size]\n",
    "        log_probs = torch.log_softmax(output, dim=1)\n",
    "        return log_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6e7efd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transforming into torch tensors for compatibility with data loaders which will be implementing batching also\n",
    "inputs_tensor = torch.tensor(inputs, dtype=torch.long)\n",
    "targets_tensor = torch.tensor(targets, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95322bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining data loader for batching\n",
    "train_data = TensorDataset(inputs_tensor, targets_tensor)\n",
    "train_loader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "143bd369",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model\n",
    "skipgram_model = SkipGramModel(VOCAB_SIZE, EMBEDDING_DIM).to(device) # to cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbb5644b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define loss function and optimizer\n",
    "criterion = nn.NLLLoss()\n",
    "skipgram_optimizer = optim.Adam(skipgram_model.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7bcf625",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training Skip-gram model (Conceptual)...\")\n",
    "for epoch in tqdm(range(NUM_EPOCHS), desc=\"Epochs\", position=0, leave=True):\n",
    "    total_loss = 0\n",
    "    \n",
    "    # DataLoader fetches batch by batch\n",
    "    for batch_inputs, batch_targets in tqdm(train_loader, desc=f\"Epoch {epoch+1} Batches\", leave=False):\n",
    "        \n",
    "        # Place data on GPU\n",
    "        batch_inputs = batch_inputs.to(device)\n",
    "        batch_targets = batch_targets.to(device)\n",
    "        \n",
    "        # Zeroing grads\n",
    "        skipgram_optimizer.zero_grad()\n",
    "        \n",
    "        # forward prop\n",
    "        log_probs = skipgram_model(batch_inputs)\n",
    "        \n",
    "        # loss calculation\n",
    "        loss = criterion(log_probs, batch_targets)\n",
    "        \n",
    "        # backward prop\n",
    "        loss.backward()\n",
    "        skipgram_optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{NUM_EPOCHS}, Loss: {total_loss/len(train_loader):.4f}\")\n",
    "print(\"Skip-gram training complete (Conceptual).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e8781ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_PATH = \"data/word_to_ix.json\"\n",
    "with open(VOCAB_PATH, 'w', encoding='utf-8') as f:\n",
    "    json.dump(word_to_ix, f, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "065472dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "SKIPGRAM_PATH = \"model/skipgram_model.pth\"\n",
    "torch.save(skipgram_model.state_dict(), SKIPGRAM_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f834da0",
   "metadata": {},
   "source": [
    "**ANN PREDICTION MODEL**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6cb533b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Skip-gram Model\n",
    "class SkipGramModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super(SkipGramModel, self).__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.linear = nn.Linear(embedding_dim, vocab_size)\n",
    "\n",
    "    def forward(self, target_word_idx):\n",
    "        # target_word_idx shape: [batch_size]\n",
    "        embed = self.embeddings(target_word_idx) # shape: [batch_size, embedding_dim]\n",
    "        output = self.linear(embed)              # shape: [batch_size, vocab_size]\n",
    "        log_probs = torch.log_softmax(output, dim=1)\n",
    "        return log_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27d488e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = 20000\n",
    "EMBEDDING_DIM = 150\n",
    "device = torch.device('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56a3b405",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49a269c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reload tokens\n",
    "with open('data/tokenized_train.pkl', 'rb') as f: # 'rb' stands for read binary\n",
    "    tokenized_sentences_train = pickle.load(f)\n",
    "\n",
    "with open('data/tokenized_test.pkl', 'rb') as f:\n",
    "    tokenized_sentences_test = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c2346fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reload index dictionary\n",
    "with open(\"data/word_to_ix.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    word_to_ix = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "587c5fa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-initialize the SkipGram model structure (Must match training config)\n",
    "loaded_skipgram = SkipGramModel(len(word_to_ix), EMBEDDING_DIM).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "485a9994",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the trained weights\n",
    "loaded_skipgram.load_state_dict(torch.load(\"model/skipgram_model.pth\"))\n",
    "loaded_skipgram.eval() # Set to evaluation mode (no gradient calculation for this part)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9997a0e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentence_embedding(sentence_tokens, model, word_to_ix, embedding_dim, device):\n",
    "    \"\"\"\n",
    "    Converts a list of tokens (sentence) into a single vector by averaging\n",
    "    the word embeddings of its constituent words.\n",
    "    \"\"\"\n",
    "    # Convert words to indices, using 0 (<UNK>) for unknown words\n",
    "    indices = [word_to_ix.get(word, 0) for word in sentence_tokens]\n",
    "    \n",
    "    # Convert to tensor and move to device\n",
    "    indices_tensor = torch.tensor(indices, dtype=torch.long).to(device)\n",
    "    \n",
    "    # Get embeddings for these words from the pre-trained Skip-gram model\n",
    "    with torch.no_grad(): # We don't need gradients here just the values\n",
    "        word_vectors = model.embeddings(indices_tensor)\n",
    "    \n",
    "    # Handle empty sentences or sentences with no known words\n",
    "    if len(word_vectors) == 0:\n",
    "        return np.zeros(embedding_dim)\n",
    "    \n",
    "    # Average the vectors to get one vector per sentence\n",
    "    # shape: [sentence_length, embedding_dim] -> [embedding_dim]\n",
    "    sentence_vector = torch.mean(word_vectors, dim=0).cpu().numpy()\n",
    "    \n",
    "    return sentence_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaf13847",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_list = [get_sentence_embedding(s, loaded_skipgram, word_to_ix, EMBEDDING_DIM, device) \n",
    "                for s in tokenized_sentences_train]\n",
    "X_test_list = [get_sentence_embedding(s, loaded_skipgram, word_to_ix, EMBEDDING_DIM, device) \n",
    "               for s in tokenized_sentences_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb57159e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert lists to NumPy arrays first then to PyTorch Tensors\n",
    "X_train_tensor = torch.tensor(np.array(X_train_list), dtype=torch.float32).to(device)\n",
    "X_test_tensor = torch.tensor(np.array(X_test_list), dtype=torch.float32).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04ef9154",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map text labels to integers\n",
    "label_mapping = {\n",
    "    'negative': 0,\n",
    "    'notr': 1,     \n",
    "    'positive': 2\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0fbe0fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply mapping to DataFrames\n",
    "df_train['label_idx'] = df_train['label'].map(label_mapping)\n",
    "df_test['label_idx'] = df_test['label'].map(label_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fb616cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare Target Tensors\n",
    "# IMPORTANT: For Multi-class CrossEntropyLoss, targets must be 1D LongTensor (int64)\n",
    "y_train_tensor = torch.tensor(df_train['label_idx'].values, dtype=torch.long).to(device)\n",
    "y_test_tensor = torch.tensor(df_test['label_idx'].values, dtype=torch.long).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "177de4bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentClassifierMLP(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_classes):\n",
    "        super(SentimentClassifierMLP, self).__init__()\n",
    "        \n",
    "        # input -> hidden\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        \n",
    "        # Activation ReLu\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        # Dropout\n",
    "        self.dropout = nn.Dropout(p=0.3) \n",
    "        \n",
    "        # hidden -> output\n",
    "        self.fc2 = nn.Linear(hidden_dim, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.dropout(out)\n",
    "        out = self.fc2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a41687f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Model\n",
    "HIDDEN_DIM = 64\n",
    "NUM_CLASSES = 3\n",
    "sentiment_model = SentimentClassifierMLP(EMBEDDING_DIM, HIDDEN_DIM, NUM_CLASSES).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b171519",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 512\n",
    "\n",
    "model_train_data = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "model_train_loader = DataLoader(model_train_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "model_test_data = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "model_test_loader = DataLoader(model_test_data, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae665491",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CrossEntropyLoss is used for multi-class classification\n",
    "criterion_cls = nn.CrossEntropyLoss()\n",
    "optimizer_cls = optim.Adam(sentiment_model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e2d68c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS_CLS = 50\n",
    "\n",
    "# Training Loop with Batches\n",
    "print(\"Starting Batch Training for Sentiment Analysis...\")\n",
    "\n",
    "for epoch in tqdm(range(EPOCHS_CLS), desc=\"Epochs\", position=0, leave=True):\n",
    "    sentiment_model.train() # Set model to training mode\n",
    "    total_loss = 0\n",
    "    \n",
    "    # Batch Loop\n",
    "    for batch_inputs, batch_targets in model_train_loader:\n",
    "        \n",
    "        # Move data to GPU/CPU\n",
    "        batch_inputs = batch_inputs.to(device)\n",
    "        batch_targets = batch_targets.to(device) # Must be torch.long dtype\n",
    "        \n",
    "        # Zero gradients\n",
    "        optimizer_cls.zero_grad()\n",
    "        \n",
    "        # Forward prop\n",
    "        outputs = sentiment_model(batch_inputs) # Shape: [Batch_Size, 3]\n",
    "        \n",
    "        # Calculate loss\n",
    "        loss = criterion(outputs, batch_targets)\n",
    "        \n",
    "        # Backward prop\n",
    "        loss.backward()\n",
    "        optimizer_cls.step()\n",
    "        \n",
    "        # Accumulate loss\n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    # Calculate average loss for the epoch\n",
    "    avg_loss = total_loss / len(model_train_loader)\n",
    "    \n",
    "    # Print every 10 epochs\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        tqdm.write(f\"Epoch {epoch+1}/{EPOCHS_CLS}, Average Loss: {avg_loss:.4f}\")\n",
    "\n",
    "print(\"Training complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fbe410b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluation Loop with Batches\n",
    "print(\"\\nStarting Evaluation on Test Set...\")\n",
    "\n",
    "sentiment_model.eval() # Set model to evaluation mode\n",
    "total_correct = 0\n",
    "total_samples = 0\n",
    "\n",
    "with torch.no_grad(): # Disable gradient calculation for efficiency\n",
    "    for batch_inputs, batch_targets in model_test_loader:\n",
    "        \n",
    "        # Move data to device\n",
    "        batch_inputs = batch_inputs.to(device)\n",
    "        batch_targets = batch_targets.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = sentiment_model(batch_inputs)\n",
    "        \n",
    "        # Get predictions: Find the index with the highest score\n",
    "        # torch.max returns (values, indices) -> we need indices (predicted class)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        \n",
    "        # Update counts\n",
    "        total_samples += batch_targets.size(0) # Batch size\n",
    "        total_correct += (predicted == batch_targets).sum().item()\n",
    "\n",
    "# Calculate final accuracy\n",
    "accuracy = (total_correct / total_samples) * 100\n",
    "print(f\"Final Test Accuracy: {accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f97c3d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses = []\n",
    "val_losses = []\n",
    "train_accuracies = []\n",
    "val_accuracies = []\n",
    "\n",
    "EPOCHS_CLS = 50\n",
    "\n",
    "print(\"Starting Training with Live Validation...\")\n",
    "\n",
    "for epoch in tqdm(range(EPOCHS_CLS), desc=\"Training Progress\"):\n",
    "    \n",
    "\n",
    "    # TRAINING PHASE\n",
    "    sentiment_model.train() # Switch to training mode\n",
    "    total_train_loss = 0\n",
    "    correct_train = 0\n",
    "    total_train_samples = 0\n",
    "    \n",
    "    for batch_inputs, batch_targets in model_train_loader:\n",
    "        # Move to device\n",
    "        batch_inputs = batch_inputs.to(device)\n",
    "        batch_targets = batch_targets.to(device)\n",
    "        \n",
    "        # Zero gradients\n",
    "        optimizer_cls.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = sentiment_model(batch_inputs)\n",
    "        loss = criterion_cls(outputs, batch_targets)\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        optimizer_cls.step()\n",
    "        \n",
    "        # Track Loss & Accuracy\n",
    "        total_train_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        correct_train += (predicted == batch_targets).sum().item()\n",
    "        total_train_samples += batch_targets.size(0)\n",
    "        \n",
    "    # Calculate average loss and accuracy for this epoch\n",
    "    avg_train_loss = total_train_loss / len(model_train_loader)\n",
    "    avg_train_acc = correct_train / total_train_samples\n",
    "    \n",
    "    # Append to history\n",
    "    train_losses.append(avg_train_loss)\n",
    "    train_accuracies.append(avg_train_acc)\n",
    "\n",
    "    # VALIDATION PHASE\n",
    "    sentiment_model.eval() # Switch to evaluation mode\n",
    "    total_val_loss = 0\n",
    "    correct_val = 0\n",
    "    total_val_samples = 0\n",
    "    \n",
    "    with torch.no_grad(): # No gradient needed for validation\n",
    "        for batch_inputs, batch_targets in model_test_loader:\n",
    "            batch_inputs = batch_inputs.to(device)\n",
    "            batch_targets = batch_targets.to(device)\n",
    "            \n",
    "            outputs = sentiment_model(batch_inputs)\n",
    "            loss = criterion_cls(outputs, batch_targets)\n",
    "            \n",
    "            total_val_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            correct_val += (predicted == batch_targets).sum().item()\n",
    "            total_val_samples += batch_targets.size(0)\n",
    "            \n",
    "    # Calculate average validation loss and accuracy\n",
    "    avg_val_loss = total_val_loss / len(model_test_loader)\n",
    "    avg_val_acc = correct_val / total_val_samples\n",
    "    \n",
    "    # Append to history\n",
    "    val_losses.append(avg_val_loss)\n",
    "    val_accuracies.append(avg_val_acc)\n",
    "    \n",
    "    # Optional: Print stats every 5 epochs\n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        tqdm.write(f\"Epoch {epoch+1} | Train Loss: {avg_train_loss:.4f} | Val Loss: {avg_val_loss:.4f} | Val Acc: {avg_val_acc:.4f}\")\n",
    "\n",
    "print(\"Training Complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd8c4115",
   "metadata": {},
   "outputs": [],
   "source": [
    "SKIPGRAM_ANN_PATH = \"model/skipgram_ann_model.pth\"\n",
    "torch.save(sentiment_model.state_dict(), SKIPGRAM_ANN_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d46ffc4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# --- Plotting the Learning Curves ---\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Plot 1: Loss Curve (Overfitting Detection)\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(train_losses, label='Train Loss', color='blue')\n",
    "plt.plot(val_losses, label='Validation (Test) Loss', color='orange')\n",
    "plt.title('Training vs Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# Plot 2: Accuracy Curve\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(train_accuracies, label='Train Accuracy', color='green')\n",
    "plt.plot(val_accuracies, label='Validation (Test) Accuracy', color='red')\n",
    "plt.title('Training vs Validation Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "116798c1",
   "metadata": {},
   "source": [
    "### FASTTEXT-ANN MODEL"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
